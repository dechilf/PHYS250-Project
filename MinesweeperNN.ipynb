{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54cec83f",
   "metadata": {},
   "source": [
    "$$\\text{Minesweeper Environment}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88540ce1",
   "metadata": {},
   "source": [
    "Required modules: NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3768fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The MinesweeperEnv Class allows the user to instantiate a game board with the parameters [rows, columns, mines]. You can reset the game board using the method game.reset(), \n",
    "# or play in the existing game with game.step(move) where move is an integer, representing the flattened index of the cell you want to reveal.\n",
    "# This version is configured for play by algorithms that interact directly with the game board, so the visual representations are not ideal for humans.\n",
    "\n",
    "# Major features: flood filling, so if you reveal a square with adjacent \"0s\" (no neighboring mines), those will all be revealed along with their neighbors. \n",
    "# Mask: self.board is the completed/full board, but the player/agent interacts with self.mask, which shows only the revealed squares. The initial state is a sparse\n",
    "# matrix where -1 implies mine. _neighbors adds adjacent mine sums to each 0 square to obtain a familiar minesweeper board look.\n",
    "# _obs converts back into a tensor of {0,1} matrices. Matrix N has {1 if cell == N, 0 otherwise}, giving a tensor representation of the board after _neighbor_counts applies _neighbors\n",
    "# all squares.\n",
    "# Finally, _place_mines_after_first_click ensures the user's first click is safe, a feature from standard Minesweeper. Nice since it symmetrically helps all model success rates, so \n",
    "# it is easier to compare.\n",
    "\n",
    "class MinesweeperEnv:\n",
    "    \"\"\"\n",
    "    Grid: (r x c) with m mines.\n",
    "    Observation: (H, W, 9) one-hot of VISIBLE counts 0..8; covered cells are all zeros.\n",
    "    Actions: flat index [0..H*W-1] = click cell (i,j). Invalid to click already revealed.\n",
    "    Rewards: +0.1 safe, +1 win, -1 lose. No extra reward for flood size.\n",
    "    \"\"\"\n",
    "    def __init__(self, rows=9, cols=9, mines=10, seed=None):\n",
    "        self.r, self.c, self.m = rows, cols, mines\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.board = np.zeros((rows, cols), dtype=np.int8)   # -1 = mine, 0 = empty\n",
    "        self.mask  = np.ones((rows, cols), dtype=np.int8)    # 1 = covered, 0 = revealed\n",
    "        self._generated = False\n",
    "        self.score = 0\n",
    "        self.explosion = False\n",
    "\n",
    "    # ---------- helpers ----------\n",
    "    def _idx(self, a): return divmod(a, self.c)\n",
    "    def _flat(self, i, j): return i*self.c + j\n",
    "\n",
    "    def _neighbors(self, i, j):\n",
    "        for di, dj in ((1,0),(0,1),(1,1),(-1,0),(0,-1),(-1,-1),(1,-1),(-1,1)):\n",
    "            x, y = i+di, j+dj\n",
    "            if 0 <= x < self.r and 0 <= y < self.c:\n",
    "                yield x, y\n",
    "\n",
    "    def _neighbor_counts(self):\n",
    "        mines = (self.board == -1).astype(np.int8)\n",
    "        p = np.pad(mines, 1)\n",
    "        return (\n",
    "            p[0:-2,0:-2] + p[0:-2,1:-1] + p[0:-2,2:] +\n",
    "            p[1:-1,0:-2]                + p[1:-1,2:] +\n",
    "            p[2:  ,0:-2] + p[2:  ,1:-1] + p[2:  ,2:]\n",
    "        )\n",
    "\n",
    "    def _place_mines_after_first_click(self, safe_i, safe_j):\n",
    "        n = self.r * self.c\n",
    "        exclude = {safe_i*self.c + safe_j}\n",
    "        assert self.m < n - 1\n",
    "        candidates = np.fromiter((k for k in range(n) if k not in exclude), dtype=np.int32)\n",
    "        mine_idx = self.rng.choice(candidates, self.m, replace=False)\n",
    "        self.board[:] = 0\n",
    "        self.board.flat[mine_idx] = -1\n",
    "        self._generated = True\n",
    "\n",
    "    def _flood_reveal(self, i, j):\n",
    "        stack = [(i, j)]\n",
    "        while stack:\n",
    "            x, y = stack.pop()\n",
    "            if self.mask[x, y] == 0 or self.board[x, y] == -1:\n",
    "                continue\n",
    "            self.mask[x, y] = 0  # reveal current\n",
    "            if self._counts[x, y] == 0:\n",
    "                # reveal all neighbors; push only zeros\n",
    "                for nx, ny in self._neighbors(x, y):\n",
    "                    if self.mask[nx, ny] == 1 and self.board[nx, ny] != -1:\n",
    "                        self.mask[nx, ny] = 0\n",
    "                        if self._counts[nx, ny] == 0:\n",
    "                            stack.append((nx, ny))\n",
    "\n",
    "\n",
    "    @property\n",
    "    def action_space_n(self):  # number of actions\n",
    "        return self.r * self.c\n",
    "\n",
    "    def legal_action_mask(self):\n",
    "        \"\"\"True for legal (covered) cells; False for revealed cells.\"\"\"\n",
    "        return (self.mask == 1).reshape(-1)\n",
    "\n",
    "    def _make_obs(self):\n",
    "        \"\"\"\n",
    "        Return (H, W, 9) one-hot of VISIBLE counts 0..8.\n",
    "        Covered cells contribute zeros across all 9 channels.\n",
    "        \"\"\"\n",
    "        covered = (self.mask == 1)\n",
    "        revealed = ~covered\n",
    "        planes = []\n",
    "        for k in range(9):  # 0..8\n",
    "            planes.append(((self._counts == k) & revealed).astype(np.float32))\n",
    "        obs = np.stack(planes, axis=-1)      # (H,W,9)\n",
    "        return obs\n",
    "\n",
    "    def reset(self):\n",
    "        self.board.fill(0)\n",
    "        self.mask.fill(1)\n",
    "        self._generated = False\n",
    "        self.score = 0\n",
    "        self.explosion = False\n",
    "        self._counts = np.zeros_like(self.board, dtype=np.int8)\n",
    "        return self._make_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        action: flat index 0..(H*W-1)\n",
    "        returns: next_obs, reward, done, info\n",
    "        \"\"\"\n",
    "        i, j = self._idx(action)\n",
    "\n",
    "        if not self._generated:\n",
    "            self._place_mines_after_first_click(i, j)\n",
    "            self._counts = self._neighbor_counts()\n",
    "\n",
    "        if self.mask[i, j] == 0:\n",
    "            return self._make_obs(), -0.05, False, {\"illegal\": True}\n",
    "\n",
    "        if self.board[i, j] == -1:\n",
    "            self.mask[i, j] = 0\n",
    "            self.explosion = True\n",
    "            return self._make_obs(), -1.0, True, {\"result\": \"lose\"}\n",
    "\n",
    "        self._flood_reveal(i, j)\n",
    "        self.score = int((self.mask == 0).sum())\n",
    "\n",
    "        if np.all(self.mask[self.board != -1] == 0):\n",
    "            return self._make_obs(), +1.0, True, {\"result\": \"win\"}\n",
    "\n",
    "        return self._make_obs(), +0.1, False, {} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacc5511",
   "metadata": {},
   "source": [
    "$$\\text{Helpers and Analytics}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96a2c35",
   "metadata": {},
   "source": [
    "Required modules: NumPy, Tensorflow\n",
    "And, must import mixed_precision from tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056d8916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Mixed precision allows the model to train faster than in the default 32-bit float.\n",
    "# LossLogger is a minimal class that uses a tensorflow feature \"Callback,\" that lets the script pull values from stages in model training. This implmenetation just\n",
    "# pulls the NN's loss at every epoch in training.\n",
    "# CSVLogger dumps other analytics into a csv file.\n",
    "\n",
    "\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "tf.config.optimizer.set_jit(True)  \n",
    "\n",
    "class LossLogger(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        print(f\"[epoch {epoch+1}] loss={logs.get('loss'):.6f}\")\n",
    "\n",
    "loss_logger = LossLogger()\n",
    "csv_logger  = tf.keras.callbacks.CSVLogger('train_log.csv', append=False)\n",
    "\n",
    "# D4_map takes advantage of symmetries in a Minesweeper board. Each cell cares about its 8 neighbors -- value in the cell is invariant under any D4 \n",
    "# mapping of the 8 neighbors. Also, a square Minesweeper board has the same symmetries, although most (e.g. single 90 degree rotation) do NOT apply to rectangular non-square boards.\n",
    "\n",
    "def d4_map(x, y, w):\n",
    "    k = tf.random.uniform([], 0, 4, dtype=tf.int32)\n",
    "    x = tf.image.rot90(x, k)\n",
    "    y = tf.image.rot90(y, k)\n",
    "    w = tf.image.rot90(w, k)\n",
    "    do_flip = tf.random.uniform([], 0, 1) < 0.5\n",
    "    x = tf.cond(do_flip, lambda: tf.image.flip_left_right(x), lambda: x)\n",
    "    y = tf.cond(do_flip, lambda: tf.image.flip_left_right(y), lambda: y)\n",
    "    w = tf.cond(do_flip, lambda: tf.image.flip_left_right(w), lambda: w)\n",
    "    return x, y, w\n",
    "\n",
    "# Generates a new board with a given seed and makes a (free) first move if not otherwise specified. This gets called in training since the current loop\n",
    "# gets unique training boards by generating from a list (i.e. 10000 boards in training could be \"for board_idx in [0, 10000]\").\n",
    "    \n",
    "def board_from_seed(env, seed, first_click=None):\n",
    "    env.rng = np.random.default_rng(seed)\n",
    "    _ = env.reset()\n",
    "    if first_click is None:\n",
    "        first_click = (env.r // 2, env.c // 2)\n",
    "    a = first_click[0] * env.c + first_click[1]\n",
    "    s, r, done, info = env.step(a)\n",
    "    return s\n",
    "\n",
    "# Gets neighbors from boolean inputs, so for example if check if mine == True, this function would compute the number of mines adjacent to a square.\n",
    "def _nb_sum_uint8(mat_bool: np.ndarray) -> np.ndarray:\n",
    "    p = np.pad(mat_bool.astype(np.uint8), 1, mode=\"constant\")\n",
    "    s = (\n",
    "        p[0:-2, 0:-2] + p[0:-2, 1:-1] + p[0:-2, 2:] +\n",
    "        p[1:-1, 0:-2] +                     p[1:-1, 2:] +\n",
    "        p[2:  , 0:-2] + p[2:  , 1:-1] + p[2:  , 2:]\n",
    "    )\n",
    "    return s\n",
    "\n",
    "# Quickly finds mines in the \"frontier\" (nonzero in the mask/player can see these) squares \n",
    "def _frontier_mask_fast(covered: np.ndarray, revealed: np.ndarray) -> np.ndarray:\n",
    "    return covered & (_nb_sum_uint8(revealed) > 0)\n",
    "\n",
    "# This function builds a dataset based on the environment.\n",
    "# Take g = MinesweeperEnv(9, 9, 10, seed): \n",
    "#           - Generated boards have 9 rows, 9 columns, 10 mines\n",
    "#           - Seeds are from a list of seeds via board_from_seed\n",
    "# Generates user specified number of board states:\n",
    "#           - A random generator picks non-mine squares in the frontier (feasible good moves for an agent) and reveals one\n",
    "#           - For each game, get N of these random guaranteed safe moves. Each time, re-initialize the game tensor.\n",
    "# Looping over this function gives a set of sets of tensors to train the model on.\n",
    "# The outputs are X (the boardstate tensors), Y (labels for if squares are mines), M (the masks for the tensors)\n",
    "\n",
    "def collect_safety_dataset(env, seeds, states_per_board=10, random_safe_steps=3):\n",
    "    H, W = env.r, env.c\n",
    "    C = 9\n",
    "    total = len(seeds) * states_per_board\n",
    "    X = np.empty((total, H, W, C), dtype=np.float32)\n",
    "    Y = np.empty((total, H, W, 1), dtype=np.float32)\n",
    "    M = np.empty((total, H, W, 1), dtype=np.float32)\n",
    "\n",
    "    rng = np.random.default_rng(0)\n",
    "    write_i = 0\n",
    "    for sd in seeds:\n",
    "        _ = board_from_seed(env, sd)\n",
    "        for _k in range(states_per_board):\n",
    "            obs = env._make_obs().astype(np.float32)\n",
    "\n",
    "            covered  = (env.mask == 1)\n",
    "            revealed = ~covered\n",
    "            frontier = _frontier_mask_fast(covered, revealed)\n",
    "\n",
    "            y = ((env.board != -1) & frontier).astype(np.float32)[..., None]\n",
    "            m = frontier.astype(np.float32)[..., None]\n",
    "\n",
    "            X[write_i] = obs\n",
    "            Y[write_i] = y\n",
    "            M[write_i] = m\n",
    "            write_i += 1\n",
    "\n",
    "            for _step in range(random_safe_steps):\n",
    "                safe_mask = (env.mask == 1) & (env.board != -1)\n",
    "                if not safe_mask.any():\n",
    "                    break\n",
    "                safe_flat = np.flatnonzero(safe_mask.ravel())\n",
    "                a = int(safe_flat[rng.integers(len(safe_flat))])\n",
    "                i, j = divmod(a, env.c)\n",
    "                env.step(i * env.c + j)\n",
    "        _ = board_from_seed(env, sd)\n",
    "    return X, Y, M\n",
    "\n",
    "# Last function combines the outputs from collect_safety_datasets into objects with tensorflow dataset type, then shuffles these. Finally, it applies the d4 mapping\n",
    "# to quickly add symmetric boardstates to the dataset. Saves time by doing this instead of generating more boards, and uses tf to speed up further with AUTOTUNE,\n",
    "# which lets tf decide how to optimize performance.\n",
    "\n",
    "def make_dataset(X, Y, W, batch=32, shuffle=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, Y, W))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=min(len(X), 10000), reshuffle_each_iteration=True)\n",
    "    ds = ds.map(d4_map, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch, drop_remainder=False).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea812e82",
   "metadata": {},
   "source": [
    "$$\\text{Graph Neural Network Model}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fa3b0f",
   "metadata": {},
   "source": [
    "Required modules: tensorflow\n",
    "\\\n",
    "From tf.keras: layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb4c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network class; methods explained inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    GNN implementation using Keras, via different CNN layers.\n",
    "    - Fully convolutional; supports variable kernel sizes and board sizes.\n",
    "    - Residual message passing using DepthwiseConv2D(3x3) + Conv2D(1x1).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels: int = 9, hidden: int = 64, depth: int = 5):\n",
    "    # Initializes with the number of input channels and depth. By default take 9 input channels since inputs are 9-tensors.\n",
    "        self.c = input_channels\n",
    "        self.hidden = hidden\n",
    "        self.depth = depth\n",
    "        self.model = None\n",
    "\n",
    "    def build(self, h=None, w=None):\n",
    "        \"\"\"\n",
    "        Builds a variable-size (None,None) model that outputs per-cell logits (H,W,1). Specifying h, w makes the model only take specific board sizes.\n",
    "        \"\"\"\n",
    "    # Takes input tensors\n",
    "        inp = layers.Input(shape=(h if h is not None else None,\n",
    "                                  w if w is not None else None,\n",
    "                                  self.c))\n",
    "    # Nx1 Convolutional layer: dimensionality reduction, converting the NxMxC tensor into an NxM matrix. Idea: use the one-hot encoded tensor to reproduce\n",
    "    # Minesweeper topography, which is a combination of the tensor layers.\n",
    "        x = layers.Conv2D(self.hidden, 1, use_bias=False)(inp)\n",
    "    # Normalization transforms Conv2D output to be close to 0 in mean, 1 in standard deviation.\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    # Using ReLU as the activation function\n",
    "        x = layers.ReLU()(x)\n",
    "    # This ends the input layer part of the network -- after this the main loop iterates for each hidden layer.\n",
    "        h = x\n",
    "        for _ in range(self.depth):\n",
    "    # 3x3 convolution acts at each hidden layer. The idea: 3x3 chunks of the inputs capture local per-cell dynamics with each cell and its neighbors.\n",
    "    # Depthwise network reduces the number of operations -- uses the 3x3 for each channel, for each cell in the input.\n",
    "    # By taking the 1x1 network, should collect the real graph nodes/embeddings, and then the 3x3 should aggregate the neighbors' features, to complete the Graph Convolutional Network.\n",
    "            y = layers.DepthwiseConv2D(3, padding=\"same\", use_bias=False)(h)\n",
    "    # Another normalization/ReLU activation\n",
    "            y = layers.BatchNormalization()(y)\n",
    "            y = layers.ReLU()(y)\n",
    "    # Additional Nx1 convolutional layer added to reproduce boardstate structure again.\n",
    "            y = layers.Conv2D(self.hidden, 1, use_bias=False)(y)\n",
    "            y = layers.BatchNormalization()(y)\n",
    "            y = layers.ReLU()(y)\n",
    "    # Takes the inputs and adds the residuals to pass them on\n",
    "            h = layers.Add()([h, y])\n",
    "\n",
    "    # 1x1 layer performs dimension reduction again to get an NxM set of logits as outputs. These logits are then converted into probabilities with Softmax.\n",
    "        out = layers.Conv2D(1, 1, padding=\"same\", activation=None)(h)\n",
    "        self.model = models.Model(inp, out)\n",
    "        return self.model\n",
    "\n",
    "class WeightedBCEFromLogits(tf.keras.losses.Loss):\n",
    "# Loss function is effectively tf's built in cross entropy with logits, which differs from cross entropy in having sigmoid built in to save computations.\n",
    "# The difference is that there is an added weight that makes loss values for squares that aren't in the frontier (visible) high. Picking these ~= guessing,\n",
    "# so the weight basically discourages guessing.\n",
    "    def __init__(self, pos_weight=3.0):\n",
    "        super().__init__(reduction=tf.keras.losses.Reduction.NONE)\n",
    "        self.pos_weight = tf.cast(pos_weight, tf.float32)\n",
    "    def call(self, y_true, y_pred):\n",
    "        return tf.nn.weighted_cross_entropy_with_logits(\n",
    "            labels=tf.cast(y_true, tf.float32),\n",
    "            logits=tf.cast(y_pred, tf.float32),\n",
    "            pos_weight=self.pos_weight\n",
    "        )\n",
    "\n",
    "def compile_gnn(model, pos_weight=3.0, lr=3e-4, use_xla=True):\n",
    "# Uses the standard optimizer and loss function to compile the model.\n",
    "    loss = WeightedBCEFromLogits(pos_weight)\n",
    "    opt = tf.keras.optimizers.Adam(lr)\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=loss,\n",
    "        jit_compile=bool(use_xla),\n",
    "        steps_per_execution=128,\n",
    "        run_eagerly=False,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_gnn(env, seeds, model, *,\n",
    "                           epochs=35, batch=128, states_per_board=6, steps_per_state=4,\n",
    "                           pos_weight=3.0, lr=3e-4, use_xla=True,\n",
    "                           loss_logger=None, csv_logger=None):\n",
    "    # Builds the datasets as before, then compiles the model.\n",
    "    X, Y, W = collect_safety_dataset(env, seeds, states_per_board, steps_per_state)\n",
    "    X = X.astype(\"float16\"); Y = Y.astype(\"float16\"); W = W.astype(\"float16\")\n",
    "    ds = make_dataset(X, Y, W, batch=batch, shuffle=True)\n",
    "    model = compile_gnn(model, pos_weight=pos_weight, lr=lr, use_xla=use_xla)\n",
    "    callbacks = []\n",
    "    if loss_logger is not None: \n",
    "        callbacks.append(loss_logger)\n",
    "    if csv_logger is not None: \n",
    "        callbacks.append(csv_logger)\n",
    "    # After initializing the analytics, this fits the model to the data over specified number of epochs, and prints the loss/saves the csv.\n",
    "    model.fit(ds, epochs=epochs, verbose=1, callbacks=callbacks)\n",
    "    return model\n",
    "\n",
    "# Uses sigmoid to convert model outputs (logits) to probabilities. These prepresent P[Cell != Mine] for each cell in the board.\n",
    "def predict(model, X):\n",
    "    logits = model(X, training=False)\n",
    "    return tf.math.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4cceb8",
   "metadata": {},
   "source": [
    "$$\\text{Testing loop}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3964db",
   "metadata": {},
   "source": [
    "Required modules: NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d150a05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function uses the neural network outputs -- which are per-cell probabilities -- to predict where mines are.\n",
    "# The algo pulls all probabilities of each cell not being a mine, then gets the highest, i.e. the safest cell, and digs it. Then, it \n",
    "# re-computes all of the logits and probabilities in the new board state. \n",
    "# Note: shoud try to be careful to avoid seeds the model trained on -- doesn't say anything about generalization and gives weird behavior.\n",
    "def greedy_gnn_algo(env, model, seeds, max_moves=None, tta=0, verbose=False):\n",
    "    wins = 0; H, W = env.r, env.c\n",
    "    # Plays for each seed given\n",
    "    for sd in seeds:\n",
    "        _ = board_from_seed(env, sd); done = False; moves = 0\n",
    "        while not done:\n",
    "        # Gets board state\n",
    "            s = env._make_obs()[None, ...].astype(\"float32\")\n",
    "        # NN predictions on board state\n",
    "            p_tf = predict(model, s, tta=tta) \n",
    "        # Only considers non-guesses/adjacent squares. Downside: rarely the only available frontier educated guess might be worse than a pure guess.\n",
    "            p = p_tf[0, ..., 0].numpy()           \n",
    "            covered  = (env.mask == 1)\n",
    "            revealed = ~covered\n",
    "            frontier = _frontier_mask_fast(covered, revealed)\n",
    "            cand = frontier & covered                        \n",
    "            if not cand.any(): \n",
    "                cand = covered\n",
    "            scores = np.where(cand, p, -1e9)              \n",
    "        # Gets highest probability of nonmine cell\n",
    "            i, j = np.unravel_index(np.argmax(scores), scores.shape)\n",
    "            _, _, done, info = env.step(i * W + j); moves += 1\n",
    "            if max_moves and moves >= max_moves: break\n",
    "        if verbose: \n",
    "            print(info.get(\"result\"))\n",
    "        wins += 1 if info.get(\"result\") == \"win\" else 0\n",
    "    # Ouputs the proportion of wins to games played\n",
    "    return wins / len(seeds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0470aad",
   "metadata": {},
   "source": [
    "$$\\text{Run the script}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0ceb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model; input channels should be 9, hidden and depth up to user\n",
    "\n",
    "from calendar import c\n",
    "\n",
    "\n",
    "nn = NeuralNetwork(input_channels=9, hidden=64, depth=5)\n",
    "\n",
    "# Builds the model; h, w = None lets the model train on any board size. Setting these --> easier to separate trained models.\n",
    "model = nn.build(h=None, w=None)\n",
    "# Train on 10x10 boards: these make it easy to compute mine densities, and are relatively not many computations. Large boards \n",
    "# increase work, and make the loss surface take longer to traverse --> much longer training. Impact on winrate for models ~E[Guesses] \n",
    "# so larger boards on similar densities should monotonically decrease winrates for deterministic and stochastic models.\n",
    "# To play with variable or different boardstates, would recommend adding other tuples to the list: (16, 16, [40]) is intermediate mode, for example.\n",
    "# The inner list is the set of densities the model trains on.\n",
    "\n",
    "train_schedules = [(10,10,[10,15,20,25])]\n",
    "for (rows, cols, mine_list) in train_schedules:\n",
    "    for mines in mine_list:\n",
    "        env = MinesweeperEnv(rows=rows, cols=cols, mines=mines, seed=0)\n",
    "        train_gnn(env, list(range(0,25000)), model,\n",
    "                               epochs=10, batch=128,\n",
    "                               states_per_board=7, steps_per_state=6,\n",
    "                               pos_weight=3.0, lr=1e-3, use_xla=True,\n",
    "                               loss_logger=loss_logger, csv_logger=csv_logger)\n",
    "\n",
    "\n",
    "rang = np.arange(20, 60)\n",
    "winrates = []\n",
    "rows = 10\n",
    "cols = 10\n",
    "\n",
    "# Plays Minesweeper over a range of mine densities and returns winrates at each density\n",
    "for m in rang:\n",
    "    env = MinesweeperEnv(rows=rows, cols=cols, mines=m, seed=123)\n",
    "    wr_gnn = greedy_gnn_algo(env, model, list(range(20000,20100)))\n",
    "    winrates.append(wr_gnn)\n",
    "    print(f\"{rows}x{cols}, mines={m}: winrate={wr_gnn:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
